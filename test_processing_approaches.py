"""
Test script to compare file-based vs chunk-based processing approaches.
"""

import os
import time
from rag.merged_pipeline import (
    run_merged_rag_pipeline,
    run_chunk_based_rag_pipeline,
    run_hybrid_rag_pipeline
)

def test_processing_approaches():
    """Test and compare different processing approaches."""
    
    # Test directory - update this path
    test_directory = r"C:\Users\22bcscs055\Downloads\mock_data"
    
    if not os.path.exists(test_directory):
        print(f"âŒ Test directory not found: {test_directory}")
        print("Please update the path in the script")
        return
    
    # Count files for context
    file_count = len([f for f in os.listdir(test_directory) 
                     if os.path.isfile(os.path.join(test_directory, f))])
    
    print("ğŸ§ª TESTING RAG PROCESSING APPROACHES")
    print("=" * 60)
    print(f"ğŸ“ Directory: {test_directory}")
    print(f"ğŸ“„ Files: {file_count}")
    print("=" * 60)
    
    # Test parameters
    max_workers = 4
    chunk_size_kb = 4
    
    results = {}
    
    # Approach 1: File-based processing
    print("\nğŸ”¹ Testing File-Based Processing")
    print("-" * 40)
    start_time = time.time()
    
    try:
        file_stats = run_merged_rag_pipeline(
            directory_path=test_directory,
            max_workers=max_workers,
            chunk_size_kb=chunk_size_kb,
            files_per_batch=5
        )
        
        file_duration = time.time() - start_time
        results['file_based'] = {
            'stats': file_stats,
            'duration': file_duration,
            'status': 'success'
        }
        
        print(f"âœ… File-based processing completed in {file_duration:.2f}s")
        
    except Exception as e:
        print(f"âŒ File-based processing failed: {e}")
        results['file_based'] = {
            'status': 'failed',
            'error': str(e)
        }
    
    # Wait a bit between approaches
    time.sleep(2)
    
    # Approach 2: Chunk-based processing
    print("\nğŸ”¸ Testing Chunk-Based Processing")
    print("-" * 40)
    start_time = time.time()
    
    try:
        chunk_stats = run_chunk_based_rag_pipeline(
            directory_path=test_directory,
            max_workers=max_workers,
            chunk_size_kb=chunk_size_kb,
            chunks_per_batch=50
        )
        
        chunk_duration = time.time() - start_time
        results['chunk_based'] = {
            'stats': chunk_stats,
            'duration': chunk_duration,
            'status': 'success'
        }
        
        print(f"âœ… Chunk-based processing completed in {chunk_duration:.2f}s")
        
    except Exception as e:
        print(f"âŒ Chunk-based processing failed: {e}")
        results['chunk_based'] = {
            'status': 'failed',
            'error': str(e)
        }
    
    # Wait a bit between approaches
    time.sleep(2)
    
    # Approach 3: Hybrid (auto-select)
    print("\nğŸ”· Testing Hybrid Processing (Auto-Select)")
    print("-" * 40)
    start_time = time.time()
    
    try:
        hybrid_stats = run_hybrid_rag_pipeline(
            directory_path=test_directory,
            max_workers=max_workers,
            chunk_size_kb=chunk_size_kb,
            files_per_batch=5,
            chunks_per_batch=50,
            use_chunk_based=None  # Auto-select
        )
        
        hybrid_duration = time.time() - start_time
        results['hybrid'] = {
            'stats': hybrid_stats,
            'duration': hybrid_duration,
            'status': 'success'
        }
        
        print(f"âœ… Hybrid processing completed in {hybrid_duration:.2f}s")
        
    except Exception as e:
        print(f"âŒ Hybrid processing failed: {e}")
        results['hybrid'] = {
            'status': 'failed',
            'error': str(e)
        }
    
    # Compare results
    print("\n" + "=" * 60)
    print("ğŸ“Š COMPARISON RESULTS")
    print("=" * 60)
    
    for approach, result in results.items():
        print(f"\nğŸ” {approach.replace('_', ' ').title()}:")
        
        if result['status'] == 'success':
            stats = result['stats']
            duration = result['duration']
            
            print(f"   â±ï¸  Duration: {duration:.2f}s")
            print(f"   ğŸ“„ Files: {stats.get('files_processed', 0)}")
            print(f"   ğŸ§© Chunks: {stats.get('chunks_created', 0)}")
            print(f"   ğŸ“¦ Stored: {stats.get('embeddings_stored', 0)}")
            print(f"   âŒ Errors: {stats.get('errors', 0)}")
            
            # Calculate throughput
            if duration > 0:
                chunks_per_sec = stats.get('chunks_created', 0) / duration
                print(f"   ğŸš€ Throughput: {chunks_per_sec:.1f} chunks/sec")
        else:
            print(f"   âŒ Failed: {result.get('error', 'Unknown error')}")
    
    # Recommendations
    print(f"\nğŸ’¡ RECOMMENDATIONS:")
    print(f"   â€¢ File count: {file_count}")
    if file_count <= 10:
        print(f"   â€¢ Recommended: File-based (fewer files, simpler coordination)")
    elif file_count <= 50:
        print(f"   â€¢ Recommended: Either approach works well")
    else:
        print(f"   â€¢ Recommended: Chunk-based (many files, better load balancing)")
    
    print(f"   â€¢ Use hybrid mode for automatic selection based on file count")

def demo_chunk_processing():
    """Demonstrate chunk-based processing features."""
    
    print("\n" + "=" * 60)
    print("ğŸ§© CHUNK-BASED PROCESSING DEMO")
    print("=" * 60)
    
    print("Key Features:")
    print("âœ… Better load balancing across workers")
    print("âœ… Handles files with varying chunk counts")
    print("âœ… More granular progress tracking")
    print("âœ… Efficient for many small files")
    print("âœ… Separates I/O from compute operations")
    
    print(f"\nProcessing Flow:")
    print(f"1. ğŸ“– Read all files sequentially (extract chunks)")
    print(f"2. ğŸ“¦ Group chunks into batches (e.g., 50 chunks/batch)")
    print(f"3. ğŸ§µ Distribute batches across worker threads")
    print(f"4. âš¡ Workers process: embed â†’ store in parallel")
    print(f"5. ğŸ“Š Aggregate results from all workers")
    
    print(f"\nBest Use Cases:")
    print(f"â€¢ Many small JSON files with varying sizes")
    print(f"â€¢ Mixed file types (JSON + text)")
    print(f"â€¢ When load balancing is important")
    print(f"â€¢ Large datasets with uneven file sizes")

if __name__ == "__main__":
    demo_chunk_processing()
    print("\n" + "ğŸ”¥" * 20)
    test_processing_approaches()